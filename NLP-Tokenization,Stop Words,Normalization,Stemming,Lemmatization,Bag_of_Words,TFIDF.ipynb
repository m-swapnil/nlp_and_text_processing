{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9bc8e5",
   "metadata": {},
   "source": [
    "# Text Mining and NLP Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81ef25",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53b92df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e703c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Tokenization is a fundamental step in natural language processing that breaks down a sentence into individual components, such as words, punctuation marks, and sometimes phrases, to make text easier to analyze and process by computers.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef31568",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.split()\n",
    "re.sub(r'([^\\s\\w]|_)+',' ',sentence).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb0884",
   "metadata": {},
   "source": [
    "## Extracting n-grams\n",
    "## n-grams can be extracted from 3 different techniques:\n",
    "### 1. Custom defined function\n",
    "### 2. NLTK\n",
    "### 3. TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a694c5",
   "metadata": {},
   "source": [
    "## Extracting n-grams using customed defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fceda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f893e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_extractor(input_str,n):\n",
    "    tokens = re.sub(r'([^\\s\\w]|_)+',' ',input_str).split()\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        print(tokens[i:i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a1ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_extractor('The cute little boy is playing with the kitten.',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4195e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_extractor('The cute little boy is playing with the kitten.',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92735c",
   "metadata": {},
   "source": [
    "## Extracting n-grams using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d84ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff4a17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(\"I am reading NLP Fundamentals\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3069cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams('The cute little boy is playing with the kitten.'.split(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db776be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams('The cute little boy is playing with the kitten.'.split(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa645ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(\"The cute little boy is playing with the kitten.\".split(),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b9f00",
   "metadata": {},
   "source": [
    "## Extracting n-grams using TextBlob\n",
    "### TextBlob is  a library used in python for processing textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f486a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"The cute little boy is playing with the kitten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8398163",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.ngrams(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81b3a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.ngrams(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f182b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.ngrams(n =4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b7705",
   "metadata": {},
   "source": [
    "## Tokenization using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d16d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The Indian-American scientist was chosen for the award from a list of nearly 50,000 nominations. The Padma Shri is Indiaâ€™s fourth largest civilian award and was given to Kak owing to his research in #AI and #cryptography.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed67c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text_to_word_sequence(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58369a92",
   "metadata": {},
   "source": [
    "## Tokenize sentences using other nltk tokenizers:\n",
    "### 1.Tweet Tokenizer\n",
    "### 2. MWE Tokenizer(Multi-Word Expression)\n",
    "### 3. Regexp Tokenizer\n",
    "### 4. Whitespace Tokenizer\n",
    "### 5. Word Punct Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f471f1",
   "metadata": {},
   "source": [
    "## 1. Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0720f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokenizer.tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236aa7c4",
   "metadata": {},
   "source": [
    "## 2. MWE Tokenizer (Multi-Word Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee389e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508f5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer([('Indian-American'),('Padma','Shri')])\n",
    "mwe_tokenizer.tokenize(sentence1.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db010c",
   "metadata": {},
   "source": [
    "## 3. RegExp Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "482efad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "reg_tokenizer.tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4fb90b",
   "metadata": {},
   "source": [
    "## 4. Whitespace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4657802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wh_tokenizer = WhitespaceTokenizer()\n",
    "wh_tokenizer.tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29faa38b",
   "metadata": {},
   "source": [
    "## 5. WordPunct Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39940fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "wp_tokenizer = WordPunctTokenizer()\n",
    "wp_tokenizer.tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad25e3a",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e74a9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"We are learning NLP in Python. Python is a very useful tool in DS. We love NLP \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db037564",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55090857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4e80e",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cbcc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8a67e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "print(stop_words)\n",
    "len(stop_words) # There are 179 predefined English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5f4202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"I am learning NLP. It is one of the most popular library in Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "383314a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words = word_tokenize(sentence2)\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d7159",
   "metadata": {},
   "source": [
    "## Filtering stop words from the input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80a71425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no_stops = ' '.join([word for word in sentence_words if word not in stop_words])\n",
    "print(sentence_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf50eb",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e049a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words in string\n",
    "sentence3 = \"I visited NY from IND on 31-12-24\"\n",
    "\n",
    "normalized_sentence = sentence3.replace(\"NY\", \"New York\").replace(\"IND\",\"India\").replace(\"-24\",\"-2024\")\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed448259",
   "metadata": {},
   "source": [
    "## Spelling Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "563b7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33a472fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller(lang = 'en')\n",
    "help(Speller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59588a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell(\"Natureal\") # Correct spelling is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05b5932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")\n",
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccce89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_corrected = ' '.join([spell(word) for word in sent1])\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b175a3",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4672b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47190953",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"Programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2f873e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"Programs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5172fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"Jumping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b656de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"Jumps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f82d1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"battling\") # battl is not an actual word as stemming doesn't check in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f29b4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('amazing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653c7f4",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "def55f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = \"Before eating, it would be nice to sanitize your hands with a sanitizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d68c7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54d9c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_stemmer = PorterStemmer()\n",
    "' '.join([ps_stemmer.stem(wd) for wd in sent2.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219ca61",
   "metadata": {},
   "source": [
    "### Regexp Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0195d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent3 = \"I love playing Cricket. Cricket players practice hard in their inning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca049133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex_stemmer = RegexpStemmer('ing$')\n",
    "' '.join([regex_stemmer.stem(wd) for wd in sent3.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9458a58",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24c12b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b88ede29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73ce1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"Programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5cd9bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('Programs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfb84f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('battling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4430249",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('amazing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52b8d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthy.nltk_wrappers import lemmatize_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37b2ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent4 = \"The codes executed today are far better than what we execute generally.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7e2f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_sent(sent4)\n",
    "words, lemmas, tags = zip(*lemmatize_sent(sent4))\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b07751",
   "metadata": {},
   "source": [
    "## Singularize and Pluralize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d189bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce55961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = TextBlob('She sells seashells on the seashore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62d97650",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "967af369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d9fa5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5.words[5].pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1c3ee",
   "metadata": {},
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f59bf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Spanish to English\n",
    "from textblob import TextBlob\n",
    "en_blob = TextBlob(u'muy bien')\n",
    "en_blob.translate(from_lang = 'es', to = 'en') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a0545",
   "metadata": {},
   "source": [
    "## Custom Stop Words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2444868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c4d2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = \"She sells seashells on the seashore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a0ee8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_word_list = ['she' , 'on' , 'the' , 'am' ,'is','not']\n",
    "' '.join([word for word in word_tokenize(sent5) if word.lower() not in custom_stop_word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36df869",
   "metadata": {},
   "source": [
    "## Extracting general features from raw texts\n",
    "\n",
    "#### Number of words\n",
    "#### Detect presence of wh words\n",
    "#### Polarity\n",
    "#### Subjectivity\n",
    "#### Language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc8dcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([['The vaccine for covid-19 will be announced on 1st August.'],\n",
    "                   ['Do you know how much expectation the world population is having from this research?'],\n",
    "                   ['This risk of virus will end on 31st July.']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9fcaf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f69a4a",
   "metadata": {},
   "source": [
    "## Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e23651a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "df['number_of_words'] = df['text'].apply(lambda x : len(TextBlob(x).words))\n",
    "df['number_of_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb9c5e",
   "metadata": {},
   "source": [
    "### Detect presence of wh words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0bf7e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_words = set(['why', 'who', 'which', 'what', 'where', 'when', 'how'])\n",
    "df['are_wh_words_present'] = df['text'].apply(lambda x: True if len(set(TextBlob(str(x)).words).intersection(wh_words)) > 0 else False)\n",
    "df['are_wh_words_present']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a49d7",
   "metadata": {},
   "source": [
    "### Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10e60275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'] = df['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "df['polarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acff98e",
   "metadata": {},
   "source": [
    "### Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2adf0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subjectivity'] = df['text'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
    "df['subjectivity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57eecec",
   "metadata": {},
   "source": [
    "### Language Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f141acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# pip install spacy\n",
    "# pip install spacy-langdetect\n",
    "\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "# Input text\n",
    "# text = 'This is an English text.'\n",
    "text = 'muy bien'\n",
    "\n",
    "# Detect the language\n",
    "detected_language = detect(text)  # Returns the language code\n",
    "detected_probabilities = detect_langs(text)  # Returns probabilities\n",
    "\n",
    "print(f\"Detected Language: {detected_language}\")\n",
    "print(f\"Probabilities: {detected_probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd44748",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3e9c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['At least seven Indian pharma companies are working to develop a vaccine against coronavirus',\n",
    "'the deadly virus that has already infected more than 14 million globally.',\n",
    "'Bharat Biotech, Indian Immunologicals, are among the domestic pharma firms working on the coronavirus vaccines in India.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa1e6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_model = CountVectorizer()\n",
    "print(bag_of_words_model.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d26f98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_df = pd.DataFrame(bag_of_words_model.fit_transform(corpus).todense())\n",
    "bag_of_words_df.columns = sorted(bag_of_words_model.vocabulary_)\n",
    "bag_of_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bdb96a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer for top 5 frequent terms\n",
    "bag_of_words_model_small = CountVectorizer(max_features=5)\n",
    "\n",
    "# Transform the corpus into a Bag of Words representation\n",
    "bag_of_words_matrix = bag_of_words_model_small.fit_transform(corpus)\n",
    "\n",
    "# Convert the matrix to a DataFrame\n",
    "bag_of_words_df_small = pd.DataFrame(bag_of_words_matrix.todense(), \n",
    "                                      columns=bag_of_words_model_small.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows\n",
    "print(bag_of_words_df_small.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec98271",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "39e78b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fba928bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "print(tfidf_model.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "421c357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(corpus).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01fa801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF for top 5 frequent terms\n",
    "tfidf_model_small = TfidfVectorizer(max_features = 5)\n",
    "tfidf_df_small = pd.DataFrame(tfidf_model_small.fit_transform(corpus).todense())\n",
    "tfidf_df_small.columns = sorted(tfidf_model_small.vocabulary_)\n",
    "tfidf_df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc404e11",
   "metadata": {},
   "source": [
    "## Feature Engineering (Text Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "998b0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from earthy.nltk_wrappers import lemmatize_sent\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f838605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair1 = [\"Do you have Covid-19\",\"Your body temperature will tell you\"]\n",
    "pair2 = [\"I travelled to Malaysia.\", \"Where did you travel?\"]\n",
    "pair3 = [\"He is a programmer\", \"Is he not a programmer?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3641995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_similarity_jaccard (text1, text2):\n",
    "    words_text1 = tuple(zip(*lemmatize_sent(text1.lower())))[1]\n",
    "    words_text2 = tuple(zip(*lemmatize_sent(text2.lower())))[1]\n",
    "    nr = len(set(words_text1).intersection(set(words_text2)))\n",
    "    dr = len(set(words_text1).union(set(words_text2)))\n",
    "    jaccard_sim = nr/dr\n",
    "    return jaccard_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed2707ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_similarity_jaccard(pair1[0], pair1[1])\n",
    "extract_text_similarity_jaccard(pair2[0], pair2[1])\n",
    "extract_text_similarity_jaccard(pair3[0], pair3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71351e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c8b293ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a corpus which will have texts of pair1, pair2 and pair3 respectively\n",
    "corpus = [pair1[0], pair1[1], pair2[0], pair2[1], pair3[0], pair3[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4ffb2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results = tfidf_model.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9dad6bbc",
   "metadata": {},
   "source": [
    "#######################################\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523c682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
