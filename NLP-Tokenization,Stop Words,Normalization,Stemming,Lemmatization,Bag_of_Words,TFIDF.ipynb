{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9bc8e5",
   "metadata": {},
   "source": [
    "# Text Mining and NLP Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81ef25",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53b92df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e703c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Tokenization is a fundamental step in natural language processing that breaks down a sentence into individual components, such as words, punctuation marks, and sometimes phrases, to make text easier to analyze and process by computers.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef31568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fundamental',\n",
       " 'step',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'that',\n",
       " 'breaks',\n",
       " 'down',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'into',\n",
       " 'individual',\n",
       " 'components',\n",
       " 'such',\n",
       " 'as',\n",
       " 'words',\n",
       " 'punctuation',\n",
       " 'marks',\n",
       " 'and',\n",
       " 'sometimes',\n",
       " 'phrases',\n",
       " 'to',\n",
       " 'make',\n",
       " 'text',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'and',\n",
       " 'process',\n",
       " 'by',\n",
       " 'computers']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()\n",
    "re.sub(r'([^\\s\\w]|_)+',' ',sentence).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb0884",
   "metadata": {},
   "source": [
    "## Extracting n-grams\n",
    "## n-grams can be extracted from 3 different techniques:\n",
    "### 1. Custom defined function\n",
    "### 2. NLTK\n",
    "### 3. TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a694c5",
   "metadata": {},
   "source": [
    "## Extracting n-grams using customed defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fceda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f893e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_extractor(input_str,n):\n",
    "    tokens = re.sub(r'([^\\s\\w]|_)+',' ',input_str).split()\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        print(tokens[i:i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a1ede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cute']\n",
      "['cute', 'little']\n",
      "['little', 'boy']\n",
      "['boy', 'is']\n",
      "['is', 'playing']\n",
      "['playing', 'with']\n",
      "['with', 'the']\n",
      "['the', 'kitten']\n"
     ]
    }
   ],
   "source": [
    "n_gram_extractor('The cute little boy is playing with the kitten.',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4195e5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cute', 'little']\n",
      "['cute', 'little', 'boy']\n",
      "['little', 'boy', 'is']\n",
      "['boy', 'is', 'playing']\n",
      "['is', 'playing', 'with']\n",
      "['playing', 'with', 'the']\n",
      "['with', 'the', 'kitten']\n"
     ]
    }
   ],
   "source": [
    "n_gram_extractor('The cute little boy is playing with the kitten.',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92735c",
   "metadata": {},
   "source": [
    "## Extracting n-grams using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d84ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff4a17fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'Fundamentals']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"I am reading NLP Fundamentals\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3069cb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The',),\n",
       " ('cute',),\n",
       " ('little',),\n",
       " ('boy',),\n",
       " ('is',),\n",
       " ('playing',),\n",
       " ('with',),\n",
       " ('the',),\n",
       " ('kitten.',)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams('The cute little boy is playing with the kitten.'.split(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db776be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'cute'),\n",
       " ('cute', 'little'),\n",
       " ('little', 'boy'),\n",
       " ('boy', 'is'),\n",
       " ('is', 'playing'),\n",
       " ('playing', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'kitten.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams('The cute little boy is playing with the kitten.'.split(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa645ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'cute', 'little'),\n",
       " ('cute', 'little', 'boy'),\n",
       " ('little', 'boy', 'is'),\n",
       " ('boy', 'is', 'playing'),\n",
       " ('is', 'playing', 'with'),\n",
       " ('playing', 'with', 'the'),\n",
       " ('with', 'the', 'kitten.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(\"The cute little boy is playing with the kitten.\".split(),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b9f00",
   "metadata": {},
   "source": [
    "## Extracting n-grams using TextBlob\n",
    "### TextBlob is  a library used in python for processing textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f486a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"The cute little boy is playing with the kitten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8398163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['The', 'cute']),\n",
       " WordList(['cute', 'little']),\n",
       " WordList(['little', 'boy']),\n",
       " WordList(['boy', 'is']),\n",
       " WordList(['is', 'playing']),\n",
       " WordList(['playing', 'with']),\n",
       " WordList(['with', 'the']),\n",
       " WordList(['the', 'kitten'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81b3a4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['The', 'cute', 'little']),\n",
       " WordList(['cute', 'little', 'boy']),\n",
       " WordList(['little', 'boy', 'is']),\n",
       " WordList(['boy', 'is', 'playing']),\n",
       " WordList(['is', 'playing', 'with']),\n",
       " WordList(['playing', 'with', 'the']),\n",
       " WordList(['with', 'the', 'kitten'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f182b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['The', 'cute', 'little', 'boy']),\n",
       " WordList(['cute', 'little', 'boy', 'is']),\n",
       " WordList(['little', 'boy', 'is', 'playing']),\n",
       " WordList(['boy', 'is', 'playing', 'with']),\n",
       " WordList(['is', 'playing', 'with', 'the']),\n",
       " WordList(['playing', 'with', 'the', 'kitten'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n =4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b7705",
   "metadata": {},
   "source": [
    "## Tokenization using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d16d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The Indian-American scientist was chosen for the award from a list of nearly 50,000 nominations. The Padma Shri is Indiaâ€™s fourth largest civilian award and was given to Kak owing to his research in #AI and #cryptography.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed67c110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'indian',\n",
       " 'american',\n",
       " 'scientist',\n",
       " 'was',\n",
       " 'chosen',\n",
       " 'for',\n",
       " 'the',\n",
       " 'award',\n",
       " 'from',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'nearly',\n",
       " '50',\n",
       " '000',\n",
       " 'nominations',\n",
       " 'the',\n",
       " 'padma',\n",
       " 'shri',\n",
       " 'is',\n",
       " 'indiaâ€™s',\n",
       " 'fourth',\n",
       " 'largest',\n",
       " 'civilian',\n",
       " 'award',\n",
       " 'and',\n",
       " 'was',\n",
       " 'given',\n",
       " 'to',\n",
       " 'kak',\n",
       " 'owing',\n",
       " 'to',\n",
       " 'his',\n",
       " 'research',\n",
       " 'in',\n",
       " 'ai',\n",
       " 'and',\n",
       " 'cryptography']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text_to_word_sequence(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58369a92",
   "metadata": {},
   "source": [
    "## Tokenize sentences using other nltk tokenizers:\n",
    "### 1.Tweet Tokenizer\n",
    "### 2. MWE Tokenizer(Multi-Word Expression)\n",
    "### 3. Regexp Tokenizer\n",
    "### 4. Whitespace Tokenizer\n",
    "### 5. Word Punct Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f471f1",
   "metadata": {},
   "source": [
    "## 1. Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0720f93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Indian-American',\n",
       " 'scientist',\n",
       " 'was',\n",
       " 'chosen',\n",
       " 'for',\n",
       " 'the',\n",
       " 'award',\n",
       " 'from',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'nearly',\n",
       " '50,000',\n",
       " 'nominations',\n",
       " '.',\n",
       " 'The',\n",
       " 'Padma',\n",
       " 'Shri',\n",
       " 'is',\n",
       " 'India',\n",
       " 'â€™',\n",
       " 's',\n",
       " 'fourth',\n",
       " 'largest',\n",
       " 'civilian',\n",
       " 'award',\n",
       " 'and',\n",
       " 'was',\n",
       " 'given',\n",
       " 'to',\n",
       " 'Kak',\n",
       " 'owing',\n",
       " 'to',\n",
       " 'his',\n",
       " 'research',\n",
       " 'in',\n",
       " '#AI',\n",
       " 'and',\n",
       " '#cryptography',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokenizer.tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236aa7c4",
   "metadata": {},
   "source": [
    "## 2. MWE Tokenizer (Multi-Word Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee389e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508f5e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Indian-American',\n",
       " 'scientist',\n",
       " 'was',\n",
       " 'chosen',\n",
       " 'for',\n",
       " 'the',\n",
       " 'award',\n",
       " 'from',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'nearly',\n",
       " '50,000',\n",
       " 'nominations.',\n",
       " 'The',\n",
       " 'Padma_Shri',\n",
       " 'is',\n",
       " 'Indiaâ€™s',\n",
       " 'fourth',\n",
       " 'largest',\n",
       " 'civilian',\n",
       " 'award',\n",
       " 'and',\n",
       " 'was',\n",
       " 'given',\n",
       " 'to',\n",
       " 'Kak',\n",
       " 'owing',\n",
       " 'to',\n",
       " 'his',\n",
       " 'research',\n",
       " 'in',\n",
       " '#AI',\n",
       " 'and',\n",
       " '#cryptography.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwe_tokenizer = MWETokenizer([('Indian-American'),('Padma','Shri')])\n",
    "mwe_tokenizer.tokenize(sentence1.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db010c",
   "metadata": {},
   "source": [
    "## 3. RegExp Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "482efad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Indian',\n",
       " '-American',\n",
       " 'scientist',\n",
       " 'was',\n",
       " 'chosen',\n",
       " 'for',\n",
       " 'the',\n",
       " 'award',\n",
       " 'from',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'nearly',\n",
       " '50',\n",
       " ',000',\n",
       " 'nominations',\n",
       " '.',\n",
       " 'The',\n",
       " 'Padma',\n",
       " 'Shri',\n",
       " 'is',\n",
       " 'India',\n",
       " 'â€™s',\n",
       " 'fourth',\n",
       " 'largest',\n",
       " 'civilian',\n",
       " 'award',\n",
       " 'and',\n",
       " 'was',\n",
       " 'given',\n",
       " 'to',\n",
       " 'Kak',\n",
       " 'owing',\n",
       " 'to',\n",
       " 'his',\n",
       " 'research',\n",
       " 'in',\n",
       " '#AI',\n",
       " 'and',\n",
       " '#cryptography.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "reg_tokenizer.tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4fb90b",
   "metadata": {},
   "source": [
    "## 4. Whitespace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4657802e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Indian-American',\n",
       " 'scientist',\n",
       " 'was',\n",
       " 'chosen',\n",
       " 'for',\n",
       " 'the',\n",
       " 'award',\n",
       " 'from',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'nearly',\n",
       " '50,000',\n",
       " 'nominations.',\n",
       " 'The',\n",
       " 'Padma',\n",
       " 'Shri',\n",
       " 'is',\n",
       " 'Indiaâ€™s',\n",
       " 'fourth',\n",
       " 'largest',\n",
       " 'civilian',\n",
       " 'award',\n",
       " 'and',\n",
       " 'was',\n",
       " 'given',\n",
       " 'to',\n",
       " 'Kak',\n",
       " 'owing',\n",
       " 'to',\n",
       " 'his',\n",
       " 'research',\n",
       " 'in',\n",
       " '#AI',\n",
       " 'and',\n",
       " '#cryptography.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wh_tokenizer = WhitespaceTokenizer()\n",
    "wh_tokenizer.tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29faa38b",
   "metadata": {},
   "source": [
    "## 5. WordPunct Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39940fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Indian',\n",
       " '-',\n",
       " 'American',\n",
       " 'scientist',\n",
       " 'was',\n",
       " 'chosen',\n",
       " 'for',\n",
       " 'the',\n",
       " 'award',\n",
       " 'from',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'nearly',\n",
       " '50',\n",
       " ',',\n",
       " '000',\n",
       " 'nominations',\n",
       " '.',\n",
       " 'The',\n",
       " 'Padma',\n",
       " 'Shri',\n",
       " 'is',\n",
       " 'India',\n",
       " 'â€™',\n",
       " 's',\n",
       " 'fourth',\n",
       " 'largest',\n",
       " 'civilian',\n",
       " 'award',\n",
       " 'and',\n",
       " 'was',\n",
       " 'given',\n",
       " 'to',\n",
       " 'Kak',\n",
       " 'owing',\n",
       " 'to',\n",
       " 'his',\n",
       " 'research',\n",
       " 'in',\n",
       " '#',\n",
       " 'AI',\n",
       " 'and',\n",
       " '#',\n",
       " 'cryptography',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "wp_tokenizer = WordPunctTokenizer()\n",
    "wp_tokenizer.tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad25e3a",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e74a9a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are learning NLP in Python.',\n",
       " 'Python is a very useful tool in DS.',\n",
       " 'We love NLP']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"We are learning NLP in Python. Python is a very useful tool in DS. We love NLP \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db037564",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55090857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Swapnil Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('Fundamentals', 'NNS')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4e80e",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cbcc551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Swapnil\n",
      "[nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8a67e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "print(stop_words)\n",
    "len(stop_words) # There are 179 predefined English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5f4202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"I am learning NLP. It is one of the most popular library in Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "383314a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'NLP', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'library', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "sentence_words = word_tokenize(sentence2)\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d7159",
   "metadata": {},
   "source": [
    "## Filtering stop words from the input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80a71425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning NLP . It one popular library Python\n"
     ]
    }
   ],
   "source": [
    "sentence_no_stops = ' '.join([word for word in sentence_words if word not in stop_words])\n",
    "print(sentence_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf50eb",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e049a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited New York from India on 31-12-2024\n"
     ]
    }
   ],
   "source": [
    "# Replace words in string\n",
    "sentence3 = \"I visited NY from IND on 31-12-24\"\n",
    "\n",
    "normalized_sentence = sentence3.replace(\"NY\", \"New York\").replace(\"IND\",\"India\").replace(\"-24\",\"-2024\")\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed448259",
   "metadata": {},
   "source": [
    "## Spelling Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "563b7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33a472fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Speller in module autocorrect:\n",
      "\n",
      "class Speller(builtins.object)\n",
      " |  Speller(lang='en', threshold=0, nlp_data=None, fast=False, only_replacements=False)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__ = autocorrect_sentence(self, sentence)\n",
      " |  \n",
      " |  __init__(self, lang='en', threshold=0, nlp_data=None, fast=False, only_replacements=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  autocorrect_sentence(self, sentence)\n",
      " |  \n",
      " |  autocorrect_word(self, word)\n",
      " |      most likely correction for everything up to a double typo\n",
      " |  \n",
      " |  existing(self, words)\n",
      " |      {'the', 'teh'} => {'the'}\n",
      " |  \n",
      " |  get_candidates(self, word)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spell = Speller(lang = 'en')\n",
    "help(Speller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59588a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(\"Natureal\") # Correct spelling is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05b5932f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ntural', 'Luanguage', 'Processin', 'deals', 'with', 'the', 'art', 'of', 'extracting', 'insightes', 'from', 'Natural', 'Languaes']\n"
     ]
    }
   ],
   "source": [
    "sent1 = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")\n",
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccce89e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing deals with the art of extracting insights from Natural Languages\n"
     ]
    }
   ],
   "source": [
    "sentence_corrected = ' '.join([spell(word) for word in sent1])\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b175a3",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4672b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47190953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'program'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"Programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2f873e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'program'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"Programs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5172fd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"Jumping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b656de47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"Jumps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f82d1933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battl'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"battling\") # battl is not an actual word as stemming doesn't check in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f29b4ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amaz'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('amazing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653c7f4",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "def55f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = \"Before eating, it would be nice to sanitize your hands with a sanitizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d68c7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54d9c7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'befor eating, it would be nice to sanit your hand with a sanit'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_stemmer = PorterStemmer()\n",
    "' '.join([ps_stemmer.stem(wd) for wd in sent2.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219ca61",
   "metadata": {},
   "source": [
    "### Regexp Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0195d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent3 = \"I love playing Cricket. Cricket players practice hard in their inning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca049133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love play Cricket. Cricket players practice hard in their inn'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex_stemmer = RegexpStemmer('ing$')\n",
    "' '.join([regex_stemmer.stem(wd) for wd in sent3.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9458a58",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24c12b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Swapnil\n",
      "[nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b88ede29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73ce1cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Programming'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"Programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5cd9bd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Programs'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('Programs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfb84f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battling'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('battling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4430249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amazing'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('amazing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52b8d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from earthy.nltk_wrappers import lemmatize_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37b2ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent4 = \"The codes executed today are far better than what we execute generally.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7e2f94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The',\n",
       " 'code',\n",
       " 'execute',\n",
       " 'today',\n",
       " 'be',\n",
       " 'far',\n",
       " 'good',\n",
       " 'than',\n",
       " 'what',\n",
       " 'we',\n",
       " 'execute',\n",
       " 'generally',\n",
       " '.')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_sent(sent4)\n",
    "words, lemmas, tags = zip(*lemmatize_sent(sent4))\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b07751",
   "metadata": {},
   "source": [
    "## Singularize and Pluralize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d189bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce55961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = TextBlob('She sells seashells on the seashore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62d97650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['She', 'sells', 'seashells', 'on', 'the', 'seashore'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "967af369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seashell'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d9fa5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seashores'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5.words[5].pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1c3ee",
   "metadata": {},
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f59bf958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"very good\")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Spanish to English\n",
    "from textblob import TextBlob\n",
    "en_blob = TextBlob(u'muy bien')\n",
    "en_blob.translate(from_lang = 'es', to = 'en') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a0545",
   "metadata": {},
   "source": [
    "## Custom Stop Words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2444868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c4d2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = \"She sells seashells on the seashore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a0ee8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sells seashells seashore'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stop_word_list = ['she' , 'on' , 'the' , 'am' ,'is','not']\n",
    "' '.join([word for word in word_tokenize(sent5) if word.lower() not in custom_stop_word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36df869",
   "metadata": {},
   "source": [
    "## Extracting general features from raw texts\n",
    "\n",
    "#### Number of words\n",
    "#### Detect presence of wh words\n",
    "#### Polarity\n",
    "#### Subjectivity\n",
    "#### Language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc8dcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([['The vaccine for covid-19 will be announced on 1st August.'],\n",
    "                   ['Do you know how much expectation the world population is having from this research?'],\n",
    "                   ['This risk of virus will end on 31st July.']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9fcaf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The vaccine for covid-19 will be announced on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you know how much expectation the world pop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This risk of virus will end on 31st July.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The vaccine for covid-19 will be announced on ...\n",
       "1  Do you know how much expectation the world pop...\n",
       "2          This risk of virus will end on 31st July."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['text']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f69a4a",
   "metadata": {},
   "source": [
    "## Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e23651a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10\n",
       "1    14\n",
       "2     9\n",
       "Name: number_of_words, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "df['number_of_words'] = df['text'].apply(lambda x : len(TextBlob(x).words))\n",
    "df['number_of_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb9c5e",
   "metadata": {},
   "source": [
    "### Detect presence of wh words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0bf7e591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "Name: are_wh_words_present, dtype: bool"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_words = set(['why', 'who', 'which', 'what', 'where', 'when', 'how'])\n",
    "df['are_wh_words_present'] = df['text'].apply(lambda x: True if len(set(TextBlob(str(x)).words).intersection(wh_words)) > 0 else False)\n",
    "df['are_wh_words_present']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a49d7",
   "metadata": {},
   "source": [
    "### Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10e60275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.2\n",
       "2    0.0\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'] = df['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "df['polarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acff98e",
   "metadata": {},
   "source": [
    "### Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2adf0e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.2\n",
       "2    0.0\n",
       "Name: subjectivity, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subjectivity'] = df['text'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
    "df['subjectivity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57eecec",
   "metadata": {},
   "source": [
    "### Language Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f141acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: es\n",
      "Probabilities: [es:0.9999951671528979]\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# pip install spacy\n",
    "# pip install spacy-langdetect\n",
    "\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "# Input text\n",
    "# text = 'This is an English text.'\n",
    "text = 'muy bien'\n",
    "\n",
    "# Detect the language\n",
    "detected_language = detect(text)  # Returns the language code\n",
    "detected_probabilities = detect_langs(text)  # Returns probabilities\n",
    "\n",
    "print(f\"Detected Language: {detected_language}\")\n",
    "print(f\"Probabilities: {detected_probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd44748",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3e9c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['At least seven Indian pharma companies are working to develop a vaccine against coronavirus',\n",
    "'the deadly virus that has already infected more than 14 million globally.',\n",
    "'Bharat Biotech, Indian Immunologicals, are among the domestic pharma firms working on the coronavirus vaccines in India.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa1e6299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1]\n",
      " [1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0]\n",
      " [0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 2 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "bag_of_words_model = CountVectorizer()\n",
    "print(bag_of_words_model.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d26f98e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>14</th>\n",
       "      <th>against</th>\n",
       "      <th>already</th>\n",
       "      <th>among</th>\n",
       "      <th>are</th>\n",
       "      <th>at</th>\n",
       "      <th>bharat</th>\n",
       "      <th>biotech</th>\n",
       "      <th>companies</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>...</th>\n",
       "      <th>pharma</th>\n",
       "      <th>seven</th>\n",
       "      <th>than</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>vaccine</th>\n",
       "      <th>vaccines</th>\n",
       "      <th>virus</th>\n",
       "      <th>working</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   14  against  already  among  are  at  bharat  biotech  companies  \\\n",
       "0   0        1        0      0    1   1       0        0          1   \n",
       "1   1        0        1      0    0   0       0        0          0   \n",
       "2   0        0        0      1    1   0       1        1          0   \n",
       "\n",
       "   coronavirus  ...  pharma  seven  than  that  the  to  vaccine  vaccines  \\\n",
       "0            1  ...       1      1     0     0    0   1        1         0   \n",
       "1            0  ...       0      0     1     1    1   0        0         0   \n",
       "2            1  ...       1      0     0     0    2   0        0         1   \n",
       "\n",
       "   virus  working  \n",
       "0      0        1  \n",
       "1      1        0  \n",
       "2      0        1  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_df = pd.DataFrame(bag_of_words_model.fit_transform(corpus).todense())\n",
    "bag_of_words_df.columns = sorted(bag_of_words_model.vocabulary_)\n",
    "bag_of_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bdb96a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  coronavirus  indian  the  working\n",
      "0    1            1       1    0        1\n",
      "1    0            0       0    1        0\n",
      "2    1            1       1    2        1\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer for top 5 frequent terms\n",
    "bag_of_words_model_small = CountVectorizer(max_features=5)\n",
    "\n",
    "# Transform the corpus into a Bag of Words representation\n",
    "bag_of_words_matrix = bag_of_words_model_small.fit_transform(corpus)\n",
    "\n",
    "# Convert the matrix to a DataFrame\n",
    "bag_of_words_df_small = pd.DataFrame(bag_of_words_matrix.todense(), \n",
    "                                      columns=bag_of_words_model_small.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows\n",
    "print(bag_of_words_df_small.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec98271",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "39e78b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fba928bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.30300252 0.         0.         0.23044123 0.30300252\n",
      "  0.         0.         0.30300252 0.23044123 0.         0.30300252\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.23044123 0.         0.30300252 0.         0.\n",
      "  0.         0.23044123 0.30300252 0.         0.         0.\n",
      "  0.30300252 0.30300252 0.         0.         0.23044123]\n",
      " [0.29388386 0.         0.29388386 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.29388386 0.\n",
      "  0.         0.         0.29388386 0.29388386 0.         0.\n",
      "  0.         0.         0.29388386 0.         0.29388386 0.29388386\n",
      "  0.         0.         0.         0.29388386 0.29388386 0.22350625\n",
      "  0.         0.         0.         0.29388386 0.        ]\n",
      " [0.         0.         0.         0.25644739 0.19503485 0.\n",
      "  0.25644739 0.25644739 0.         0.19503485 0.         0.\n",
      "  0.25644739 0.25644739 0.         0.         0.25644739 0.25644739\n",
      "  0.25644739 0.19503485 0.         0.         0.         0.\n",
      "  0.25644739 0.19503485 0.         0.         0.         0.39006971\n",
      "  0.         0.         0.25644739 0.         0.19503485]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "print(tfidf_model.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "421c357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>14</th>\n",
       "      <th>against</th>\n",
       "      <th>already</th>\n",
       "      <th>among</th>\n",
       "      <th>are</th>\n",
       "      <th>at</th>\n",
       "      <th>bharat</th>\n",
       "      <th>biotech</th>\n",
       "      <th>companies</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>...</th>\n",
       "      <th>pharma</th>\n",
       "      <th>seven</th>\n",
       "      <th>than</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>vaccine</th>\n",
       "      <th>vaccines</th>\n",
       "      <th>virus</th>\n",
       "      <th>working</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230441</td>\n",
       "      <td>0.303003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303003</td>\n",
       "      <td>0.230441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230441</td>\n",
       "      <td>0.303003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303003</td>\n",
       "      <td>0.303003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.223506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256447</td>\n",
       "      <td>0.195035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256447</td>\n",
       "      <td>0.256447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         14   against   already     among       are        at    bharat  \\\n",
       "0  0.000000  0.303003  0.000000  0.000000  0.230441  0.303003  0.000000   \n",
       "1  0.293884  0.000000  0.293884  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.256447  0.195035  0.000000  0.256447   \n",
       "\n",
       "    biotech  companies  coronavirus  ...    pharma     seven      than  \\\n",
       "0  0.000000   0.303003     0.230441  ...  0.230441  0.303003  0.000000   \n",
       "1  0.000000   0.000000     0.000000  ...  0.000000  0.000000  0.293884   \n",
       "2  0.256447   0.000000     0.195035  ...  0.195035  0.000000  0.000000   \n",
       "\n",
       "       that       the        to   vaccine  vaccines     virus   working  \n",
       "0  0.000000  0.000000  0.303003  0.303003  0.000000  0.000000  0.230441  \n",
       "1  0.293884  0.223506  0.000000  0.000000  0.000000  0.293884  0.000000  \n",
       "2  0.000000  0.390070  0.000000  0.000000  0.256447  0.000000  0.195035  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(corpus).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01fa801f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>indian</th>\n",
       "      <th>the</th>\n",
       "      <th>working</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.353553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        are  coronavirus    indian       the   working\n",
       "0  0.500000     0.500000  0.500000  0.000000  0.500000\n",
       "1  0.000000     0.000000  0.000000  1.000000  0.000000\n",
       "2  0.353553     0.353553  0.353553  0.707107  0.353553"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF for top 5 frequent terms\n",
    "tfidf_model_small = TfidfVectorizer(max_features = 5)\n",
    "tfidf_df_small = pd.DataFrame(tfidf_model_small.fit_transform(corpus).todense())\n",
    "tfidf_df_small.columns = sorted(tfidf_model_small.vocabulary_)\n",
    "tfidf_df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc404e11",
   "metadata": {},
   "source": [
    "## Feature Engineering (Text Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "998b0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from earthy.nltk_wrappers import lemmatize_sent\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f838605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair1 = [\"Do you have Covid-19\",\"Your body temperature will tell you\"]\n",
    "pair2 = [\"I travelled to Malaysia.\", \"Where did you travel?\"]\n",
    "pair3 = [\"He is a programmer\", \"Is he not a programmer?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3641995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_similarity_jaccard (text1, text2):\n",
    "    words_text1 = tuple(zip(*lemmatize_sent(text1.lower())))[1]\n",
    "    words_text2 = tuple(zip(*lemmatize_sent(text2.lower())))[1]\n",
    "    nr = len(set(words_text1).intersection(set(words_text2)))\n",
    "    dr = len(set(words_text1).union(set(words_text2)))\n",
    "    jaccard_sim = nr/dr\n",
    "    return jaccard_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed2707ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text_similarity_jaccard(pair1[0], pair1[1])\n",
    "extract_text_similarity_jaccard(pair2[0], pair2[1])\n",
    "extract_text_similarity_jaccard(pair3[0], pair3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71351e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c8b293ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a corpus which will have texts of pair1, pair2 and pair3 respectively\n",
    "corpus = [pair1[0], pair1[1], pair2[0], pair2[1], pair3[0], pair3[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4ffb2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results = tfidf_model.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9dad6bbc",
   "metadata": {},
   "source": [
    "#######################################\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523c682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
